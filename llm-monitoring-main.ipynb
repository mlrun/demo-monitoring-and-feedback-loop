{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4668db48-ebef-4529-ad64-f40435e82153",
   "metadata": {},
   "source": [
    "# Large Language Model Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b5106-61db-4852-9778-bc7109051c38",
   "metadata": {},
   "source": [
    "Maintaining the performance of machine learning models in production is essential. Model monitoring tracks key metrics like accuracy, latency, and resource usage, to identify issues such as data drift and model decay. LLMs, like any other models, need to be monitored! \n",
    "\n",
    "In this demo, we'll develop a banking chatbot. At first the bot will answer any question in any subject. We will monitor, fine-tune and redploy it to make it more secure for answering only banking related questions. In order to do so, we'll build an automated feedback loop from detecting accuracy drift, retraining and redeployment.\n",
    "\n",
    "This notebook guides you through setting up an effective model monitoring system that leverages LLMs (LLM as a Judge) to maintain high standards for deployed models. It demonstrates how to prepare and evaluate a good prompt for the LLM judge, deploy model monitoring applications, assess the performance of a pre-trained model, fine-tune it using the ORPO technique on the supplied dataset, show the monitoring results for the fine-tuned model and finally, set an automatic pipeline to automatically fine-tune the model once the monitor raised an alert.\n",
    "\n",
    "![](./images/feedback_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bbe5a-cd74-4386-aafe-80f93a0e5339",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [LLM as a Judge](#llm-as-a-judge)\n",
    "3. [MLRun's Model Monitoring](#mlrun-model-monitoring)\n",
    "4. [ORPO Fine-tuning](#orpo-fine-tuning)\n",
    "5. [Automated Feedback Loop](#automated-feedback-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3e1b0-1ef7-4b48-94a1-3a537b3cbb7f",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1. Install and Import Requirements\n",
    "\n",
    "This demo uses the following python packages:\n",
    "* [mlrun](https://www.mlrun.org/) - Iguazio's MLRun to orchestrate the entire demo.\n",
    "* [openai](https://openai.com/) - OpenAI's ChatGPT as the LLM Judge.\n",
    "* [transformers](https://huggingface.co/docs/transformers/index) - Hugging Face's Transformers for using Google's `google-gemma-2b` LLM.\n",
    "* [datasets](https://huggingface.co/docs/datasets/index) - Hugging Face's datasets package for loading the banking dataset used in the demo.\n",
    "* [trl](https://huggingface.co/docs/trl/index) - Hugging Face's TRL for the ORPO fine-tuning.\n",
    "* [peft](https://huggingface.co/docs/peft/index) - Hugging Face's PEFT for the LORA adapter fine-tuning.\n",
    "* [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index) - Hugging Face's BitsAndBytes for loading the LLM\n",
    "* [sentencepiece](https://github.com/google/sentencepiece) - Google's tokenizer for Gemma-2B.\n",
    "\n",
    "> Note: This demo uses the gemma-2b model by Google. This model is publicly accessible, but if you want to use it then you \n",
    "    have to first read and accept its terms and conditions. Alternatively, look for a different model and change the \n",
    "    code of this demo.\n",
    "    \n",
    "> Note: Running this demo requires an available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a7305-00ff-4cfc-acb4-cac53eb2505e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T09:43:16.775855Z",
     "start_time": "2025-01-30T09:43:13.140647Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "%pip install -U -r requirements.txt\n",
    "if sys.version_info.major == 3 and sys.version_info.minor == 9:\n",
    "    !pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c99e64-b5a2-45c8-83f3-eda2e0d79cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import dotenv   \n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import shutil\n",
    "import mlrun\n",
    "from mlrun.features import Feature  # To log the model with inputs and outputs information\n",
    "import mlrun.common.schemas.alert as alert_constants  # To configure an alert\n",
    "from mlrun.model_monitoring.helpers import get_result_instance_fqn  # To configure an alert\n",
    "\n",
    "from src.llm_as_a_judge import OpenAIJudge\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b5468-baed-4fa9-af38-9c58da507895",
   "metadata": {},
   "source": [
    "### 1.2. Set Credentials\n",
    "\n",
    "* **Hugging Face** Access Token can be created and used from the account settings [access tokens](https://huggingface.co/settings/tokens). \n",
    "* **OpenAI** Secret API key can be found on the [API key page](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e6527-3f48-46d9-b1f3-870ae9a7fe33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dotenv.load_dotenv() #you can create a .env file with the following variables, HF_TOKEN, OPENAI_API_KEY, OPENAI_BASE_URL\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83120de-f797-49f8-be8e-da00b5111f14",
   "metadata": {},
   "source": [
    "### 1.3. Create an MLRun Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964b81f-4ba6-4436-a9fb-6690d157f684",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the project:\n",
    "project = mlrun.get_or_create_project(\n",
    "    name=\"llm-monitoring\",\n",
    "    parameters={\"image\":\".llm-serving\",\n",
    "        \"node_selector\": None, # Change to a node selector that is used in GPUs nodes\n",
    "    },\n",
    "    user_project = True,\n",
    "    context=\"./src\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc0d35-a963-4df9-a91b-0a3d4b25576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.datastore.datastore_profile import DatastoreProfileV3io\n",
    "\n",
    "v3io_profile = mlrun.datastore.DatastoreProfileV3io(\n",
    "        name=\"v3io-model-monitoring\",\n",
    "        v3io_access_key=mlrun.mlconf.get_v3io_access_key(),\n",
    "    )\n",
    "project.register_datastore_profile(v3io_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085940f9-f354-49cc-8817-eb91b3193d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy all the real-time monitoring functions:\n",
    "project.set_model_monitoring_credentials(stream_profile_name=v3io_profile.name,tsdb_profile_name=v3io_profile.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84430c13-34fa-4b6e-9a48-f00b0a6baf27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project.enable_model_monitoring(\n",
    "    image=\"mlrun/mlrun\",\n",
    "    base_period=2,  # frequency (in minutes) at which the monitoring applications are triggered\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221853f-f743-4748-af10-033698ee6c27",
   "metadata": {},
   "source": [
    "<a id=\"llm-as-a-judge\"></a>\n",
    "## 2. LLM as a Judge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efdd45-1edb-463f-bc7f-1f82b8dd4134",
   "metadata": {},
   "source": [
    "Using LLMs as judges for model monitoring is an innovative approach that leverages their remarkable language understanding capabilities. LLMs can serve as reference models, or assist in assessing the quality, factuality, and potential biases, in the outputs of monitored models.\n",
    "\n",
    "We will have 2 attempts to prompt engineer ChatGPT to be our judge. But first, let's get an evaluation set and an accuracy measurment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fa9e3-3b67-44f5-845a-228e106ebe5e",
   "metadata": {},
   "source": [
    "### 2.1. Load the Banking Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011439f-c945-4836-87be-5310fea63e59",
   "metadata": {},
   "source": [
    "First use a small dataset to teach the model to answer only banking related questions. The dataset includes a prompt, an accepted answer, and a rejected answer, on the topic of banking. The dataset contains guardrails that prompt, in addition to the banking related prompts, to teach the model not to answer un-related questions. \n",
    "\n",
    "> This dataset is also used later to train the model using ORPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5524deb-f4c1-4bf2-8a28-d58f0f382402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mlrun/banking-orpo-new\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098f434-10cd-4bc4-87d2-e497b3223eea",
   "metadata": {},
   "source": [
    "Preview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fd074-b3ce-46ae-9220-94893b51acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21e5e7-6f19-4353-becf-781437ec2462",
   "metadata": {},
   "source": [
    "### 2.2. Create an Accuracy Metric\n",
    "\n",
    "This simple function acts as the judge's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681b870-bc99-42d2-9fa5-5b970a4489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(col1, col2):\n",
    "    # Calculate the number of matching values\n",
    "    matching_values = sum(col1 == col2)\n",
    "\n",
    "    # Calculate the total number of values\n",
    "    total_values = len(col1)\n",
    "\n",
    "    # Calculate the percentage of matching values\n",
    "    return matching_values / total_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842034c-0693-4a23-a53a-c651801c68f9",
   "metadata": {},
   "source": [
    "### 3.3. Create the Evaluation Set\n",
    "\n",
    "To prepare the dataset for evaluation, take 10% of the data and split it into two:\n",
    "* The first portion contains questions and answers as expected, meaning that the answers are taken from the **chosen** column.\n",
    "* The second portion contains questions with unexpected answers, meaning that the answers are taken from the **rejected** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8f723-8cc1-43a5-aa27-22e75ea5179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 10% of the data:\n",
    "orpo_dataset = dataset.to_pandas().sample(frac=0.1, random_state=42, ignore_index=True)\n",
    "middle_index = len(orpo_dataset) // 2\n",
    "\n",
    "# Make 50% of the data correct and 50% of the data incorrect:\n",
    "chosen = (\n",
    "    orpo_dataset.iloc[:middle_index]\n",
    "    .rename(columns={\"prompt\": \"question\", \"chosen\": \"answer\"})\n",
    "    .drop(\"rejected\", axis=1)\n",
    ")\n",
    "rejected = (\n",
    "    orpo_dataset.iloc[middle_index:]\n",
    "    .rename(columns={\"prompt\": \"question\", \"rejected\": \"answer\"})\n",
    "    .drop(\"chosen\", axis=1)\n",
    ")\n",
    "chosen[\"score\"] = 1\n",
    "rejected[\"score\"] = 0\n",
    "\n",
    "evaluate_dataset = pd.concat([chosen, rejected])\n",
    "labels = evaluate_dataset[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61bf3e-e4d7-4cf9-9835-4ea3935d7045",
   "metadata": {},
   "source": [
    "And here is the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2f3d4-3d7d-4ceb-9392-1ee79b4108d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7a5c0-ae5e-411d-bbee-1e7637afd77f",
   "metadata": {},
   "source": [
    "### 3.4. Prompt Engineering the Judge - **First Attempt**\n",
    "\n",
    "For the first attempt, use a naive basic prompt to the judge, passing a custom string represneting the template with placeholders for `question`, `answer` and `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf26355-9c69-4bda-88ce-8849af0925c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_banking_template = \"\"\"\n",
    "1 score if the model answers for banking questions, 0 score otherwise\n",
    "The question:\n",
    "{question}\n",
    "The answer:\n",
    "{answer}\n",
    "Answer the following question and return as a python dictionary:\n",
    "{{\"score\": <a numerical score for the response>,\n",
    "\"explanation\": <a string value of an explanation about how did you determine the score>}}\n",
    "[Output]:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdba00-8cb9-4ee4-8595-ae7e46fe656b",
   "metadata": {},
   "source": [
    "Use a Judging class that uses OpenAI GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad13cf-8348-47e8-b8f5-1f810e494cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the OpenAI Judge\n",
    "judge = OpenAIJudge(\n",
    "    judge_type=\"custom-grading\",\n",
    "    metric_name=\"Restrict-to-banking\",\n",
    "    model_name=OPENAI_MODEL,\n",
    "    prompt_template=bad_banking_template,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06744e24-0d71-422c-b5e0-a9a3b3db8db6",
   "metadata": {},
   "source": [
    "Call the judge, and then the accuracy function to get the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d1f56-f084-469f-b428-258477ec32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_attempt_result = judge.judge(evaluate_dataset)\n",
    "accuracy = compute_accuracy(labels, first_attempt_result[\"score\"])\n",
    "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9756c-f350-499b-b641-8b5592065e93",
   "metadata": {},
   "source": [
    "As you can see, these results are **not good**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7a081-39e9-49eb-8d79-ae8a486b4131",
   "metadata": {},
   "source": [
    "### 3.5. Prompt Engineering the Judge - **Second Attempt**\n",
    "\n",
    "This run uses a different prompt template: `single-grading`. This template is much more detailed, so we are not using our own template, but using `prompt_config` with the following fields:\n",
    "- `name`: the name of the metric.\n",
    "- `definition`: A short definition of the metric.\n",
    "- `rubric`: The rubric explanation for each score.\n",
    "- `examples`: This section is very important. It gives examples for each rubric and cases that are covered. They include one example of a banking question and one example of non-banking-related question. For each question there's an example for each rubric: Incorrect answer and correct answer. This helps the judge to make the right decision.\n",
    "\n",
    "The judge takes the single-grading template and and fill in the values from the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83faa2af-5ed4-46f5-94d8-ae00e2c4ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_to_banking_config = {\n",
    "    \"name\": \"Restrict to banking\",\n",
    "    \"definition\": \"The Restrict to banking is a metric used to evaluate if a model responds exclusively to questions pertaining to banking topics.\",\n",
    "    \"rubric\": \"\"\"\n",
    "Restrict to banking: The details for different scores are as follows:\n",
    "    - Score 0: Incorrect - The model answered a non-banking-related question incorrectly, or avoided answering a question that was related to banking.\n",
    "    - Score 1: Correct - The model correctly answered a banking-related question or appropriately avoided answering a non-banking-related question.\n",
    "\"\"\",\n",
    "    \"examples\": \"\"\"\n",
    "Question: What is the process to apply for a mortgage?\n",
    "    Score 0: Incorrect\n",
    "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
    "    Score 1: Correct\n",
    "    Answer: \"To apply for a mortgage, you need to provide proof of income, a credit report, and a down payment. You can start by contacting a bank or a mortgage lender.\"\n",
    "Question: What is the best recipe for chocolate cake?\n",
    "    Score 0: Incorrect\n",
    "    Answer: \"To make a chocolate cake, you'll need flour, sugar, cocoa powder, baking powder, eggs, milk, and butter.\"\n",
    "    Score 1: Correct\n",
    "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
    "\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189072b-6deb-4c16-8a35-cfeb05391013",
   "metadata": {},
   "source": [
    "Now run the same process as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c5fc6-3aad-4173-97a1-221c6a3552d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = OpenAIJudge(\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"Restrict-to-banking\",\n",
    "    model_name=OPENAI_MODEL,\n",
    "    prompt_config=restrict_to_banking_config,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a47c1-9e48-406f-8a2b-3351177ed731",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_attempt_result = judge.judge(evaluate_dataset)\n",
    "accuracy = compute_accuracy(labels, second_attempt_result[\"score\"])\n",
    "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8a9e5-37aa-492b-a331-92f829dedebb",
   "metadata": {},
   "source": [
    "Now that the **LLM works well as a judge**, the next stage is the actual model monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a25133-4be7-43be-bc33-9dd68674382a",
   "metadata": {},
   "source": [
    "<a id=\"mlrun-model-monitoring\"></a>\n",
    "## 3. MLRun's Model Monitoring\n",
    "\n",
    "MLRun's model monitoring service includes built-in model monitoring and reporting capabilities. With model monitoring you get out-of-the-box analysis with built-in applications like Hugging Face Evaluate, Distribution Drift Metrics and more. For more information, click [here](https://docs.mlrun.org/en/latest/concepts/model-monitoring.html).\n",
    "\n",
    "This demo uses the custom judge application `OpenAIJudge` that was just built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7c06e-64db-4655-bd77-c7dbf0215531",
   "metadata": {},
   "source": [
    "### 3.1. Deploying the Monitoring Application\n",
    "\n",
    "First, deploy the model monitoring application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae55b44-8f13-40ca-a50a-662d9cd3fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "application = project.set_model_monitoring_function(\n",
    "    func=\"src/llm_as_a_judge.py\",\n",
    "    application_class=\"LLMAsAJudgeApplication\",\n",
    "    name=\"llm-as-a-judge\",\n",
    "    image=project.default_image,\n",
    "    framework=\"openai\",\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"restrict_to_banking\",\n",
    "    model_name=OPENAI_MODEL,\n",
    "    prompt_config=restrict_to_banking_config,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf1af0-f4cd-48c3-a9fb-8a1369748d76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "application_deployemnt = project.deploy_function(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7492a-c64e-4532-8ccd-ff09b7557554",
   "metadata": {},
   "source": [
    "### 3.2. DeepEval model monitroing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1345d-f58b-4e8b-9329-21cf428b6f9f",
   "metadata": {},
   "source": [
    "Let's have DeepEval as a judge and see the performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf13a1c-304d-445a-b8a6-46ec7f003e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepeval_application = project.set_model_monitoring_function(\n",
    "    func=\"src/deepeval_as_a_judge.py\",\n",
    "    application_class=\"DeepEvalAsAJudgeApplication\",\n",
    "    name=\"deepeval-as-a-judge\",\n",
    "    image=application_deployemnt.function.status.container_image,\n",
    "    metric_name=\"restrict_to_banking_deepeval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a4fa2-5dc1-48be-992a-fe85386d21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepeval_application_deployment = project.deploy_function(deepeval_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530bd07-8a0f-44d8-ac10-8ea39de626ac",
   "metadata": {},
   "source": [
    "### 3.3. Deploy the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd171097-960e-4971-8b2e-d2c371823fbd",
   "metadata": {},
   "source": [
    "First log it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd94444-b83e-4547-80a3-294688102af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the model to the project:\n",
    "base_model = \"google-gemma-2b\"\n",
    "project.log_model(\n",
    "    base_model,\n",
    "    model_file=\"src/model-iris.pkl\",\n",
    "    inputs=[Feature(value_type=\"str\", name=\"question\")],\n",
    "    outputs=[Feature(value_type=\"str\", name=\"answer\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e314e-290a-4adc-ad7a-608325cca4ca",
   "metadata": {},
   "source": [
    "Now, create a model server to serve this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a670975-28fe-4839-ba33-d3693a88f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serving function to evaluate the base model:\n",
    "serving_function = project.get_function(\"llm-server\")\n",
    "\n",
    "# Add the logged model:\n",
    "serving_function.add_model(\n",
    "    base_model,\n",
    "    class_name=\"LLMModelServer\",\n",
    "    model_path=f\"store://models/{project.name}/{base_model}:latest\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"max_length\": 80,\n",
    "    },\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e7beb-a2b5-4cfc-bb26-58b97ea703a0",
   "metadata": {},
   "source": [
    "To enable monitoring, use the method `set_tracking`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002712f4-1058-474e-9e86-1ce2b37a1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.set_tracking()\n",
    "serving_function.spec.readiness_timeout = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d83eae-a904-4161-bcc9-f25ed09befb4",
   "metadata": {},
   "source": [
    "And lastly, deploy it as a serverless function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43f694-762a-409c-b053-358672cb99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6db53-6514-4af6-b6c8-8eecc5043f48",
   "metadata": {},
   "source": [
    "### 3.4. Configure an Alert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b25bf-028d-40b3-aa7b-275ad190ac80",
   "metadata": {},
   "source": [
    "Define an alert to be triggered on degradation of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c4369-16c7-42b4-9057-6e623be63a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_name = \"llm-as-a-judge\"\n",
    "result_name = \"restrict-to-banking\"\n",
    "message = \"Model perf detected\"\n",
    "alert_config_name = \"restrict-to-banking\"\n",
    "dummy_url = \"dummy-webhook.default-tenant.app.llm-dev.iguazio-cd1.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee93a4-b296-42a6-9f2d-d9ed549670c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Endpoint ID:\n",
    "endpoints = mlrun.get_run_db().list_model_endpoints(project=project.name)\n",
    "ep_id = endpoints.endpoints[0].metadata.uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144ddc2-5552-4670-ba15-c21b19b4164f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prj_alert_obj = get_result_instance_fqn(\n",
    "    ep_id, app_name=app_name, result_name=result_name\n",
    ")\n",
    "\n",
    "webhook_notification = mlrun.common.schemas.Notification(\n",
    "    name=\"webhook\",\n",
    "    kind=\"webhook\",\n",
    "    params={\"url\": dummy_url},\n",
    "    when=[\"completed\", \"error\"],\n",
    "    severity=\"debug\",\n",
    "    message=\"Model perf detected\",\n",
    "    condition=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea519ff5-0d4c-4f39-bd00-57c77b54fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun.common.schemas.alert as alert_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfcf75-d01f-49c7-92da-32b22c87f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_config = mlrun.alerts.alert.AlertConfig(\n",
    "    project=project.name,\n",
    "    name=alert_config_name,\n",
    "    summary=alert_config_name,\n",
    "    severity=alert_constants.AlertSeverity.HIGH,\n",
    "    entities=alert_constants.EventEntities(\n",
    "        kind=alert_constants.EventEntityKind.MODEL_ENDPOINT_RESULT,\n",
    "        project=project.name,\n",
    "        ids=[prj_alert_obj],\n",
    "    ),\n",
    "    trigger=alert_constants.AlertTrigger(\n",
    "        events=[alert_objects.EventKind.MODEL_PERFORMANCE_DETECTED, alert_objects.EventKind.MODEL_PERFORMANCE_SUSPECTED]\n",
    "    ),\n",
    "    criteria=alert_constants.AlertCriteria(count=1, period=\"10m\"),\n",
    "    notifications=[\n",
    "        alert_constants.AlertNotification(notification=webhook_notification)\n",
    "    ],\n",
    "    reset_policy=mlrun.common.schemas.alert.ResetPolicy.MANUAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d85fb-f146-4923-9372-49a890dd25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.store_alert_config(alert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11348e6-e53a-4e5e-a680-7c18f4298316",
   "metadata": {},
   "source": [
    "### 3.5. Check the Performance of the Base Model\n",
    "\n",
    "To evaluate the base model, ask it a number of questions and give it some requests. \n",
    "\n",
    "**It's expected to fail**, since it is not trained in any way to prevent it from answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9d2b4-b000-4caf-99fb-3eea578069d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_questions = [\n",
    "    \"What is a mortgage?\",\n",
    "    \"How does a credit card work?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"Please plan me a 4-days trip to north Italy\",\n",
    "    \"Write me a song\",\n",
    "    \"How much people are there in the world?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How does the stock market work?\",\n",
    "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
    "    \"Please plan me a 3-day trip to Paris\",\n",
    "    \"Write me a poem about the ocean\",\n",
    "    \"How many continents are there in the world?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does a hybrid car work?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"Please plan me a week-long trip to New Zealand\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657e239-a049-480f-8936-752ab09e7327",
   "metadata": {},
   "source": [
    "The monitoring application is , and is activated in a set time-period. Therefore, you need to create a questioning function that is timed, and separates the questioning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2b6ba-d864-41d3-b0e2-4dbb81562a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_model(questions, serving_function, base_model):\n",
    "    for question in questions:\n",
    "        seconds = 0.5\n",
    "        # Invoking the pretrained model:\n",
    "        ret = serving_function.invoke(\n",
    "            path=f\"/v2/models/{base_model}/infer\",\n",
    "            body={\"inputs\": [question]},\n",
    "        )\n",
    "        time.sleep(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaeb589-4460-4273-b030-86430cfd9735",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(20):\n",
    "    question_model(\n",
    "        questions=example_questions,\n",
    "        serving_function=serving_function,\n",
    "        base_model=base_model,\n",
    "    )\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb33a0-971c-42f9-b5ec-afb1212ad1ba",
   "metadata": {},
   "source": [
    "The Grafana model monitoring page shows the base model's scores. You will see after 10 minutes of traffic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8d216-a456-4c2f-b3e5-b92964d0599a",
   "metadata": {},
   "source": [
    "![](./images/grafana_before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f8310-4efb-4ade-a54a-646b5af9b690",
   "metadata": {},
   "source": [
    "As you can see, the base model is not the best at answering only banking-related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80851fb2-9911-4976-8cd4-298c7a6b6938",
   "metadata": {},
   "source": [
    "### 3.6 Evaluate the model using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793d59d-8272-4771-8cd6-4ff9564bf3f4",
   "metadata": {},
   "source": [
    "Let's also see how to use DeepEval to measure the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9ca1c-f9cd-4439-8705-d5096d15c8f2",
   "metadata": {},
   "source": [
    "#### Banking related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb382d9-9a54-4407-888c-48e9174536d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    HallucinationMetric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5914b5-2ce4-4a06-aff6-1d71116ef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the process to apply for a mortgage?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1a112-705c-479a-9eb5-3a11642a1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daddbb-0f04-42f4-a7fb-83c8b30ab486",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case1 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"To apply for a mortgage, you need to provide proof of income, a credit report, and a down payment. You can start by contacting a bank or a mortgage lender.\",\n",
    "    retrieval_context=[\"For mortgage application you need to provide proof of income, a credit report, and a down payment\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric1 = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "results1 = evaluate(test_cases=[test_case1], metrics=[answer_relevancy_metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab9479-5205-4e73-9c0d-336c602b9fab",
   "metadata": {},
   "source": [
    "#### Banking non-related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fc882-b8bf-4a69-8225-6a8f1c524895",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who painted the Mona Lisa?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18b491-1576-498e-b40d-f71e3035dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1265674-2af4-4a1f-8452-d850d88df6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case2 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\",\n",
    "    context=[\"This is a banking agent that allowed to talk on banking related issues only.\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric2 = HallucinationMetric(threshold=0.5,model=OPENAI_MODEL)\n",
    "\n",
    "results2 = evaluate(test_cases=[test_case2], metrics=[answer_relevancy_metric2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92380a-727f-4c58-a5a0-9346700ead38",
   "metadata": {},
   "source": [
    "<a id=\"orpo-fine-tuning\"></a>\n",
    "## 4. ORPO Fine-tuning\n",
    "\n",
    "To fine-tune the model, take the requests sent to the model (questions related to and not related to banking), build a dataset according to the [ORPO](https://huggingface.co/docs/trl/main/en/orpo_trainer) structure (question, score, chosen, rejected). Then, re-train the model with it.\n",
    "\n",
    "The result in a fine-tuned model that only answers banking-questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16d6dc-bd0e-4db0-9d5e-2a7a9b966359",
   "metadata": {},
   "source": [
    "### 4.1. Build the Training Set\n",
    "\n",
    "First, fetch the data collected by the model monitoring from the initial traffic to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e9e40-ad6a-4b8e-8dd5-88d43f2c7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = project.list_artifacts(kind=\"dataset\")\n",
    "ds_key = datasets[0][\"spec\"][\"db_key\"]\n",
    "input_ds = f\"store://datasets/{project.name}/{ds_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb2e83",
   "metadata": {},
   "source": [
    "Now, use OpenAI ChatGPT to generate expected outputs (you can see the function [here](./src/generate_ds.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e05f11cec8df6",
   "metadata": {},
   "source": [
    "> **Note:** To upload the generated dataset you need to provide an hf repo that your account token has permission to upload datasets, for example: `hf_repo_id:mlrun/banking-orpo-new`, if None skip the dataset upload to hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.build_function(\"generate-ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bf89c-9729-4c9a-8e66-0088ff33d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = project.run_function(\n",
    "    function=\"generate-ds\",\n",
    "    handler=\"generate_ds\",\n",
    "    params={\"input_ds\": input_ds,\"hf_repo_id\":None},\n",
    "    outputs=[\"new-train-ds\", \"dataset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cb3cc-92aa-4ce1-9c0b-0dfe5a8d136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0767dea-5cd2-4a9d-ac08-a418357b916b",
   "metadata": {},
   "source": [
    "Now we have a new dataset for the model tuning stored in HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c064094-0739-4180-93a3-4873f186f995",
   "metadata": {},
   "source": [
    "### 4.2. Fine-tune the Model\n",
    "\n",
    "Now, it's time to fine-tune the model using the ORPO algorithm, so that the model only answers the banking-related questions.\n",
    "\n",
    "[ORPO](https://arxiv.org/abs/2403.07691) is a new method designed to simplify and improve the process of fine-tuning language models to align with user preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556c120-85b1-4ff5-9395-949ea4a50644",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build the train function image \n",
    "train_func = project.build_function(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105b3a2-77bd-450e-a01c-10e2a424862f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "project.run_function(\n",
    "    function=\"train\",\n",
    "    params={\n",
    "        \"dataset\": \"mlrun/banking-orpo-opt\",\n",
    "        \"base_model\": \"google/gemma-2b\",\n",
    "        \"new_model\": \"mlrun/gemma-2b-bank-v0.2\",\n",
    "        \"device\": \"cuda:0\",\n",
    "    },\n",
    "    handler=\"train\",\n",
    "    outputs=[\"model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15032f46-9dfc-4d88-87c7-610d26189f9e",
   "metadata": {},
   "source": [
    "### 4.3. Check the Performance of the Fine-tuned Model\n",
    "\n",
    "Now load and deploy the trained model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac65b0-76ff-419f-910a-6fb856b212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.add_model(\n",
    "    base_model,\n",
    "    class_name=\"LLMModelServer\",\n",
    "    llm_type=\"HuggingFace\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    adapter=\"mlrun/gemma-2b-bank-v0.2\",\n",
    "    model_path=f\"store://models/{project.name}/{base_model}:latest\",\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"max_length\": 80,\n",
    "    },\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "serving_function.set_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3364b62-8897-4037-832b-51a27490fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cf2a5-d489-4747-9c39-0645be0362d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(20):\n",
    "    question_model(\n",
    "        questions=example_questions,\n",
    "        serving_function=serving_function,\n",
    "        base_model=base_model,\n",
    "    )\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815b08b-d9de-4499-994f-9139b104954f",
   "metadata": {},
   "source": [
    "The Grafana model monitoring page shows a high pass rate and a high guardrails score:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a549dc-d19a-4895-a837-89c52cd3791f",
   "metadata": {},
   "source": [
    "![](./images/grafana_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721119e-ce10-47c7-a62f-99bc2c334ea0",
   "metadata": {},
   "source": [
    "### 4.4 Evaluate the model using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f2e7d-2d43-4548-bcda-6fd987c6dc35",
   "metadata": {},
   "source": [
    "Again, test the fine tuned model's performance using DeepEval:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452db891-cefd-4489-8423-048b6e3a2f82",
   "metadata": {},
   "source": [
    "#### Banking related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd473e-e713-4745-9a81-8a41a453f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a bank mortgage?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff6c40-b9e4-4c1c-a230-1f466c1b3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668677ff-f6ad-4a97-8b86-0af7b6d773a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case1 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"A mortgage is a loan used to purchase a house or other real estate.\",\n",
    "    retrieval_context=[\"A mortgage is a banking related term\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric1 = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "results1 = evaluate(test_cases=[test_case1], metrics=[answer_relevancy_metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe8c26-8aff-427c-b238-de12d7238646",
   "metadata": {},
   "source": [
    "#### Banking non-related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce08612-ebec-482d-a043-d17144ed2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who painted the Mona Lisa?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12313d-754d-4608-9680-da6e0bbcb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f38e7-44b2-40fc-8b94-c157fc0e3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case2 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\",\n",
    "    context=[\"This is a banking agent that allowed to talk on banking related issues only.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ff582-efe0-4247-89f0-18c9d86ee3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric2 = HallucinationMetric(threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3298fd-9c61-48fa-a85c-e170528fc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = evaluate(test_cases=[test_case2], metrics=[answer_relevancy_metric2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8b2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-demo",
   "language": "python",
   "name": "conda-env-.conda-new-demo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
